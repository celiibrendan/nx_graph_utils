{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "m65 = dj.create_virtual_module('m65', 'microns_minnie65_01')\n",
    "schema = dj.schema(\"microns_minnie65_01\")\n",
    "dj.config[\"display.limit\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class NullModelParameters(dj.Manual):\n",
    "    definition=\"\"\"\n",
    "    n\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimation_version = 0\n",
    "decimation_ratio = 0.25\n",
    "import time\n",
    "\n",
    "\n",
    "@schema\n",
    "class MultiSomaCentroidValidation(dj.Computed):\n",
    "    definition=\"\"\"\n",
    "    -> minnie.Decimation\n",
    "    soma_index : tinyint unsigned #index given to this soma to account for multiple somas in one base semgnet\n",
    "    ---\n",
    "    centroid_x           : int unsigned                 # (EM voxels)\n",
    "    centroid_y           : int unsigned                 # (EM voxels)\n",
    "    centroid_z           : int unsigned                 # (EM voxels)\n",
    "    distance_from_prediction : double                   # the distance of the ALLEN predicted centroid soma center from the algorithms prediction\n",
    "    prediction_matching_index : int unsigned            # the soma index that was used to compute the error\n",
    "    soma_vertices             : longblob                # array of vertices\n",
    "    soma_faces             : longblob                   # array of faces\n",
    "    multiplicity         : tinyint unsigned             # the number of somas found for this base segment\n",
    "    run_time : double                   # the amount of time to run (seconds)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #key_source = (minnie.Decimation & (dj.U(\"segment_id\") & (m65.AllenSegmentCentroid & \"status=1\").proj()) & \"version=\" + str(decimation_version))\n",
    "    key_source = minnie.Decimation() & (m65.AllenMultiSomas() & \"status > 0\").proj()\n",
    "    \n",
    "    def make(self,key):\n",
    "        #get the mesh data\n",
    "        print(f\"\\n\\n\\n---- Working on {key['segment_id']} ----\")\n",
    "        \n",
    "        new_mesh = (minnie.Decimation() & key).fetch1(\"mesh\")\n",
    "        current_mesh_verts,current_mesh_faces = new_mesh.vertices,new_mesh.faces\n",
    "        \n",
    "        segment_id = key[\"segment_id\"]\n",
    "        \n",
    "        total_soma_list, run_time = extract_soma_center(segment_id,current_mesh_verts,current_mesh_faces)\n",
    "        print(f\"Run time was {run_time}  and the total_soma_list = {total_soma_list} and \")\n",
    "        \n",
    "        #check if soma list is empty and did not find soma\n",
    "        if len(total_soma_list) <= 0:\n",
    "            print(\"There were no somas found for this mesh so just writing empty data\")\n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=-1,\n",
    "                              centroid_x=None,\n",
    "                               centroid_y=None,\n",
    "                               centroid_z=None,\n",
    "                               distance_from_prediction=None,\n",
    "                               prediction_matching_index = None,\n",
    "                               soma_vertices=None,\n",
    "                               soma_faces=None,\n",
    "                               multiplicity=0,\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            raise Exception(\"to prevent writing because none were found\")\n",
    "            self.insert1(insert_dict,skip_duplicates=True)\n",
    "            return\n",
    "        \n",
    "        #if there is one or more soma found\n",
    "        \n",
    "        #get the soma prediction\n",
    "        \n",
    "        \n",
    "        new_array = (m65.AllenMultiSomas.Centroids()  & key).fetch(\"centroid_x\",\"centroid_y\",\"centroid_z\")\n",
    "        soma_ids = (m65.AllenMultiSomas.Centroids()  & key).fetch(\"soma_id\")\n",
    "        allen_centroid_prediction = np.array(new_array).T\n",
    "        allen_centroid_prediction\n",
    "        #print(\"soma_ids = \" + str(soma_ids))\n",
    "        from pykdtree.kdtree import KDTree\n",
    "        mesh_tree = KDTree(allen_centroid_prediction)\n",
    "\n",
    "        \n",
    "        dicts_to_insert = []\n",
    "        \n",
    "\n",
    "            \n",
    "        for i,current_soma in enumerate(total_soma_list):\n",
    "            print(\"Trying to write off file\")\n",
    "            current_soma.export(f\"{key['segment_id']}/{key['segment_id']}_soma_{i}.off\")\n",
    "            auto_prediction_center = np.mean(current_soma.vertices,axis=0)\n",
    "            \n",
    "            distances,closest_node = mesh_tree.query(auto_prediction_center.reshape(1,3))\n",
    "            error_distance = distances[0]\n",
    "            prediction_matching_index = soma_ids[closest_node[0]] #closest nodes and the distances\n",
    "            \n",
    "            \n",
    "            insert_dict = dict(key,\n",
    "                              soma_index=i,\n",
    "                              centroid_x=auto_prediction_center[0],\n",
    "                               centroid_y=auto_prediction_center[1],\n",
    "                               centroid_z=auto_prediction_center[2],\n",
    "                               distance_from_prediction=error_distance,\n",
    "                               prediction_matching_index = prediction_matching_index,\n",
    "                               soma_vertices=current_soma.vertices,\n",
    "                               soma_faces=current_soma.faces,\n",
    "                               multiplicity=len(total_soma_list),\n",
    "                               run_time=run_time\n",
    "                              )\n",
    "            \n",
    "            \n",
    "            \n",
    "            dicts_to_insert.append(insert_dict)\n",
    "            \n",
    "        #raise Exception(\"to prevent writing\")\n",
    "        \n",
    "        if len(total_soma_list) != len(soma_ids):\n",
    "            raise Exception(\"to prevent writing SOMAS NOT EQUAL TO That registered in datase\")\n",
    "            \n",
    "        self.insert(dicts_to_insert,skip_duplicates=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
