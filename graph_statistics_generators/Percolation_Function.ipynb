{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the function that will generate\n",
    "A number for the site percolation of both random and \n",
    "degree centric attacks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_vertex_order(G,selection_type=\"random\"):\n",
    "    if selection_type == \"random\":\n",
    "        return np.random.permutation(list(G.nodes))\n",
    "    elif selection_type == \"degree\":\n",
    "        \"\"\" Will organize from highest to lowest degree\"\"\"\n",
    "        degree_dict = dict()\n",
    "        for k,v in G.degree():\n",
    "            if v not in degree_dict.keys():\n",
    "                degree_dict[v] = [k]\n",
    "            else:\n",
    "                degree_dict[v].append(k)\n",
    "        degree_dict\n",
    "\n",
    "        #get the order of degree\n",
    "        order_degrees = np.sort(list(degree_dict.keys()))\n",
    "\n",
    "        node_order = []\n",
    "        for k in order_degrees:\n",
    "            node_order += list(np.random.permutation(degree_dict[k]))\n",
    "\n",
    "        return node_order\n",
    "    else:\n",
    "        raise Exception(\"Invalid Selection Type\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_site_percolation(G,vertex_order_type=\"random\",n_iterations=1000):\n",
    "    total_runs = []\n",
    "\n",
    "    for y in tqdm(range(0,n_iterations)):\n",
    "        current_run_results = [0,1]\n",
    "        \"\"\"\n",
    "        1) Start with empty network. Number of clusters, c = 0, currently in network\n",
    "        Choose at random the order in which vertices will be added to the network\n",
    "        \"\"\"\n",
    "\n",
    "        clusters=dict() #starting out the clusters list as empyt\n",
    "        vertex_order = _get_vertex_order(G,vertex_order_type)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        2) Add the next vertex in list to the network initially with no edges\n",
    "        \"\"\"\n",
    "        vertex_labels = dict()\n",
    "        for i,v in enumerate(vertex_order):\n",
    "            #print(f\"Working on vertex {v}\")\n",
    "\n",
    "            \"\"\" 2b)\n",
    "            - increase the cluster count by 1 (because the new vertex is initially a cluster of its own)\n",
    "            - Make the cluster size of one\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                max_index_plus_1 = np.max(list(clusters.keys())) + 1\n",
    "                clusters[max_index_plus_1] = 1\n",
    "                vertex_labels[v] = max_index_plus_1\n",
    "            except:\n",
    "                clusters[0] = 1\n",
    "                vertex_labels[v] = 0\n",
    "                continue\n",
    "\n",
    "            \"\"\"\n",
    "            3) Go through the edges attached to newly added vertex and add the edges where the other \n",
    "            vertex already exists in the network\n",
    "\n",
    "            4) For each edge added, check if the vertices have the same cluster group number:\n",
    "            - if yes then do nothing\n",
    "            - if no, relabel the smaller cluster the same cluster number as the bigger cluster number\n",
    "            - update the sizes of the 2 clusters from which formed\n",
    "            \"\"\"\n",
    "            already_added_v = set(vertex_order[:i]).intersection(set(G[v].keys()))\n",
    "            for a_v in already_added_v:\n",
    "                if vertex_labels[a_v] != vertex_labels[v]:\n",
    "                    index_max = np.argmax([clusters[vertex_labels[a_v]],clusters[vertex_labels[v]]])\n",
    "                    if index_max == 0: #need to change all the labels with v\n",
    "                        replaced_cluster = vertex_labels[v]\n",
    "                        indexes_to_change = [jj for jj in vertex_labels.keys() if vertex_labels[jj] == vertex_labels[v]]\n",
    "                        final_cluster = vertex_labels[a_v]\n",
    "                    else:\n",
    "                        replaced_cluster = vertex_labels[a_v]\n",
    "                        indexes_to_change = [jj for jj in vertex_labels.keys() if vertex_labels[jj] == vertex_labels[a_v]]\n",
    "                        final_cluster = vertex_labels[v]\n",
    "\n",
    "                    #change the labels\n",
    "                    for vv in indexes_to_change:\n",
    "                        vertex_labels[vv] = final_cluster\n",
    "\n",
    "                    replaced_size = clusters.pop(replaced_cluster)\n",
    "                    clusters[final_cluster] += replaced_size\n",
    "\n",
    "            current_run_results.append(np.max([v for v in clusters.values()]))\n",
    "\n",
    "\n",
    "            #Done adding that vertex and will continue on to next vertex\n",
    "            #print(f\"clusters = {clusters}\")\n",
    "\n",
    "            total_runs.append(current_run_results)\n",
    "    total_runs = np.array(total_runs)\n",
    "    \n",
    "    from scipy.special import comb\n",
    "    n = len(G.nodes)\n",
    "    S_r = np.mean(total_runs,axis=0)\n",
    "    #calculate s_phi : average largest cluster size as a functin of the occupancy probability\n",
    "    phi = np.arange(0,1.05,0.05)\n",
    "    r = np.arange(0,n+1,1)\n",
    "    s_phi = [np.sum([comb(n, r_curr, exact=True)*(phi_curr**r_curr)*((1-phi_curr)**(n- r_curr))*S_r_curr\n",
    "                        for r_curr,S_r_curr in zip(r,S_r)]) for phi_curr in phi]\n",
    "    s_phi = np.array(s_phi)/n\n",
    "    \n",
    "    return s_phi,phi\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def random_degree_site_percolation(G,n_iterations):\n",
    "    random_degree_site_percolation.stat_names = [\"area_above_identity_random_percol\",\n",
    "                                                \"area_below_identity_random_percol\",\n",
    "                                                \"area_above_identity_degree_percol\",\n",
    "                                                \"area_below_identity_degree_percol\"]\n",
    "    s_phi_barabasi_rand,phi_barabasi_rand= run_site_percolation(G,\"random\",n_iterations)\n",
    "    s_phi_barabasi_degree,phi_barabasi_degree= run_site_percolation(G,\"degree\",n_iterations)\n",
    "\n",
    "    rand_diff = s_phi_barabasi_rand - phi_barabasi_rand\n",
    "    degree_diff = s_phi_barabasi_degree - phi_barabasi_degree\n",
    "\n",
    "    dx = phi_barabasi_rand[1]-phi_barabasi_rand[0]\n",
    "\n",
    "    rand_diff_positive = np.where(rand_diff>0)[0]\n",
    "    rand_diff_negative = np.where(rand_diff<= 0)[0]\n",
    "    degree_diff_positive = np.where(degree_diff>0)[0]\n",
    "    degree_diff_negative = np.where(degree_diff<=0)[0]\n",
    "\n",
    "    return (np.trapz(rand_diff[rand_diff_positive],dx=dx),\n",
    "     np.trapz(rand_diff[rand_diff_negative],dx=dx),\n",
    "     np.trapz(degree_diff[degree_diff_positive],dx=dx),\n",
    "     np.trapz(degree_diff[degree_diff_negative],dx=dx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 100.78it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 130.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, -0.013963357497612554, 0.0, -0.043023459125347725)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iterations = 20\n",
    "G = nx.random_graphs.watts_strogatz_graph(n=100,p=0.4,k=10)\n",
    "random_degree_site_percolation(G,n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"stat_names\" in dir(random_degree_site_percolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: 8, 5: 6, 10: 11}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {5:6,10:11}\n",
    "y = {7:8}\n",
    "y.update(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
