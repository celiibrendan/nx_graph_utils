{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the function that will generate\n",
    "A number for the site percolation of both random and \n",
    "degree centric attacks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_vertex_order(G,selection_type=\"random\"):\n",
    "    if selection_type == \"random\":\n",
    "        return np.random.permutation(list(G.nodes))\n",
    "    elif selection_type == \"degree\":\n",
    "        \"\"\" Will organize from highest to lowest degree\"\"\"\n",
    "        degree_dict = dict()\n",
    "        for k,v in G.degree():\n",
    "            if v not in degree_dict.keys():\n",
    "                degree_dict[v] = [k]\n",
    "            else:\n",
    "                degree_dict[v].append(k)\n",
    "        degree_dict\n",
    "\n",
    "        #get the order of degree\n",
    "        order_degrees = np.sort(list(degree_dict.keys()))\n",
    "\n",
    "        node_order = []\n",
    "        for k in order_degrees:\n",
    "            node_order += list(np.random.permutation(degree_dict[k]))\n",
    "\n",
    "        return node_order\n",
    "    else:\n",
    "        raise Exception(\"Invalid Selection Type\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_site_percolation(G,vertex_order_type=\"random\",n_iterations=1000):\n",
    "    total_runs = []\n",
    "\n",
    "    for y in tqdm(range(0,n_iterations)):\n",
    "        current_run_results = [0,1]\n",
    "        \"\"\"\n",
    "        1) Start with empty network. Number of clusters, c = 0, currently in network\n",
    "        Choose at random the order in which vertices will be added to the network\n",
    "        \"\"\"\n",
    "\n",
    "        clusters=dict() #starting out the clusters list as empyt\n",
    "        vertex_order = _get_vertex_order(G,vertex_order_type)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        2) Add the next vertex in list to the network initially with no edges\n",
    "        \"\"\"\n",
    "        vertex_labels = dict()\n",
    "        for i,v in enumerate(vertex_order):\n",
    "            #print(f\"Working on vertex {v}\")\n",
    "\n",
    "            \"\"\" 2b)\n",
    "            - increase the cluster count by 1 (because the new vertex is initially a cluster of its own)\n",
    "            - Make the cluster size of one\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                max_index_plus_1 = np.max(list(clusters.keys())) + 1\n",
    "                clusters[max_index_plus_1] = 1\n",
    "                vertex_labels[v] = max_index_plus_1\n",
    "            except:\n",
    "                clusters[0] = 1\n",
    "                vertex_labels[v] = 0\n",
    "                continue\n",
    "\n",
    "            \"\"\"\n",
    "            3) Go through the edges attached to newly added vertex and add the edges where the other \n",
    "            vertex already exists in the network\n",
    "\n",
    "            4) For each edge added, check if the vertices have the same cluster group number:\n",
    "            - if yes then do nothing\n",
    "            - if no, relabel the smaller cluster the same cluster number as the bigger cluster number\n",
    "            - update the sizes of the 2 clusters from which formed\n",
    "            \"\"\"\n",
    "            already_added_v = set(vertex_order[:i]).intersection(set(G[v].keys()))\n",
    "            for a_v in already_added_v:\n",
    "                if vertex_labels[a_v] != vertex_labels[v]:\n",
    "                    index_max = np.argmax([clusters[vertex_labels[a_v]],clusters[vertex_labels[v]]])\n",
    "                    if index_max == 0: #need to change all the labels with v\n",
    "                        replaced_cluster = vertex_labels[v]\n",
    "                        indexes_to_change = [jj for jj in vertex_labels.keys() if vertex_labels[jj] == vertex_labels[v]]\n",
    "                        final_cluster = vertex_labels[a_v]\n",
    "                    else:\n",
    "                        replaced_cluster = vertex_labels[a_v]\n",
    "                        indexes_to_change = [jj for jj in vertex_labels.keys() if vertex_labels[jj] == vertex_labels[a_v]]\n",
    "                        final_cluster = vertex_labels[v]\n",
    "\n",
    "                    #change the labels\n",
    "                    for vv in indexes_to_change:\n",
    "                        vertex_labels[vv] = final_cluster\n",
    "\n",
    "                    replaced_size = clusters.pop(replaced_cluster)\n",
    "                    clusters[final_cluster] += replaced_size\n",
    "\n",
    "            current_run_results.append(np.max([v for v in clusters.values()]))\n",
    "\n",
    "\n",
    "            #Done adding that vertex and will continue on to next vertex\n",
    "            #print(f\"clusters = {clusters}\")\n",
    "\n",
    "            total_runs.append(current_run_results)\n",
    "    total_runs = np.array(total_runs)\n",
    "    \n",
    "    from scipy.special import comb\n",
    "    n = len(G.nodes)\n",
    "    S_r = np.mean(total_runs,axis=0)\n",
    "    #calculate s_phi : average largest cluster size as a functin of the occupancy probability\n",
    "    phi = np.arange(0,1.05,0.05)\n",
    "    r = np.arange(0,n+1,1)\n",
    "    s_phi = [np.sum([comb(n, r_curr, exact=True)*(phi_curr**r_curr)*((1-phi_curr)**(n- r_curr))*S_r_curr\n",
    "                        for r_curr,S_r_curr in zip(r,S_r)]) for phi_curr in phi]\n",
    "    s_phi = np.array(s_phi)/n\n",
    "    \n",
    "    return s_phi,phi\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 112.88it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 121.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "n_iterations = 1000\n",
    "G = nx.random_graphs.watts_strogatz_graph(n=100,p=0.4,k=10)\n",
    "\n",
    "#def random_degree_site_percolation(G,n_iterations)\n",
    "# random_degree_site_percolation.stat_names = [\"area_above_identity_random_percol\",\n",
    "#                                             \"area_below_identity_random_percol\",\n",
    "#                                             \"area_above_identity_degree_percol\",\n",
    "#                                             \"area_below_identity_degree_percol\"]\n",
    "s_phi_barabasi_rand,phi_barabasi_rand= run_site_percolation(G,\"random\",n_iterations)\n",
    "s_phi_barabasi_degree,phi_barabasi_degree= run_site_percolation(G,\"degree\",n_iterations)\n",
    "\n",
    "rand_diff = s_phi_barabasi_rand - phi_barabasi_rand\n",
    "degree_diff = s_phi_barabasi_degree - phi_barabasi_degree\n",
    "\n",
    "dx = phi_barabasi_rand[1]-phi_barabasi_rand[0]\n",
    "\n",
    "rand_diff_positive = np.where(rand_diff>0)[0]\n",
    "rand_diff_negative = np.where(rand_diff<= 0)[0]\n",
    "degree_diff_positive = np.where(degree_diff>0)[0]\n",
    "degree_diff_negative = np.where(degree_diff<=0)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -3.00005190e-02, -5.50793578e-02, -6.17282328e-02,\n",
       "       -5.02001423e-02, -3.17397716e-02, -1.71730623e-02, -8.74713706e-03,\n",
       "       -4.44596593e-03, -2.26746148e-03, -1.12184128e-03, -4.95069714e-04,\n",
       "       -1.67086337e-04, -3.77185398e-05, -5.70369762e-06, -5.30323844e-07,\n",
       "       -1.59779957e-08, -5.41628964e-11, -3.33066907e-15, -1.11022302e-16,\n",
       "        0.00000000e+00])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "print(rand_diff_positive)\n",
    "print(rand_diff_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = [0,1, 2, 3, 4, 5]\n",
    "x = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "np.trapz(y,dx = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.trapz(y, x=None, dx=1.0, axis=-1)[source]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
